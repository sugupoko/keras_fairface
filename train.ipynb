{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75673b6c-dd2d-44ac-8238-58a2377a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref :  https://www.acceluniverse.com/blog/developers/2020/03/pythonefficientnet-multi-outpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdc76b2-9d64-43f5-9a6f-7a155c9a665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing Dataset \n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4c26ae-0299-4498-ab70-242c560069ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"fairface_label_train.csv\") # shape is (86744, 5)\n",
    "test_df = pd.read_csv(\"fairface_label_val.csv\") # shape is (10954, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d1f576-26cd-41a7-b0a7-dcf931aa8077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>service_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86739</th>\n",
       "      <td>train/86740.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86740</th>\n",
       "      <td>train/86741.jpg</td>\n",
       "      <td>10-19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86741</th>\n",
       "      <td>train/86742.jpg</td>\n",
       "      <td>more than 70</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86742</th>\n",
       "      <td>train/86743.jpg</td>\n",
       "      <td>10-19</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86743</th>\n",
       "      <td>train/86744.jpg</td>\n",
       "      <td>40-49</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86744 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file           age  gender        race  service_test\n",
       "0          train/1.jpg         50-59    Male  East Asian          True\n",
       "1          train/2.jpg         30-39  Female      Indian         False\n",
       "2          train/3.jpg           3-9  Female       Black         False\n",
       "3          train/4.jpg         20-29  Female      Indian          True\n",
       "4          train/5.jpg         20-29  Female      Indian          True\n",
       "...                ...           ...     ...         ...           ...\n",
       "86739  train/86740.jpg         20-29    Male      Indian          True\n",
       "86740  train/86741.jpg         10-19    Male      Indian          True\n",
       "86741  train/86742.jpg  more than 70  Female      Indian          True\n",
       "86742  train/86743.jpg         10-19  Female       Black          True\n",
       "86743  train/86744.jpg         40-49    Male       White         False\n",
       "\n",
       "[86744 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f947f3d-13b2-4ace-95ae-523f579acde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"age_id\"] = train_df[\"age\"].astype(\"category\").cat.codes\n",
    "train_df[\"race_id\"] = train_df[\"race\"].astype(\"category\").cat.codes\n",
    "train_df[\"gender_id\"] = train_df[\"gender\"].astype(\"category\").cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94d6b82d-06e9-4269-b965-479646aa9861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        East Asian\n",
      "1            Indian\n",
      "2             Black\n",
      "3            Indian\n",
      "4            Indian\n",
      "            ...    \n",
      "86739        Indian\n",
      "86740        Indian\n",
      "86741        Indian\n",
      "86742         Black\n",
      "86743         White\n",
      "Name: race, Length: 86744, dtype: category\n",
      "Categories (7, object): ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n"
     ]
    }
   ],
   "source": [
    "race_c = train_df[\"race\"].astype(\"category\").astype(\"category\")\n",
    "print(race_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e0d0479-2cf6-422f-bac0-66ec1eeb3d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Male\n",
      "1        Female\n",
      "2        Female\n",
      "3        Female\n",
      "4        Female\n",
      "          ...  \n",
      "86739      Male\n",
      "86740      Male\n",
      "86741    Female\n",
      "86742    Female\n",
      "86743      Male\n",
      "Name: gender, Length: 86744, dtype: category\n",
      "Categories (2, object): ['Female', 'Male']\n"
     ]
    }
   ],
   "source": [
    "gender_c = train_df[\"gender\"].astype(\"category\").astype(\"category\")\n",
    "print(gender_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c31495ea-4d7c-419d-9bd9-e16f370f9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               50-59\n",
      "1               30-39\n",
      "2                 3-9\n",
      "3               20-29\n",
      "4               20-29\n",
      "             ...     \n",
      "86739           20-29\n",
      "86740           10-19\n",
      "86741    more than 70\n",
      "86742           10-19\n",
      "86743           40-49\n",
      "Name: age, Length: 86744, dtype: category\n",
      "Categories (9, object): ['0-2', '10-19', '20-29', '3-9', ..., '40-49', '50-59', '60-69', 'more than 70']\n"
     ]
    }
   ],
   "source": [
    "age_c = train_df[\"age\"].astype(\"category\").astype(\"category\")\n",
    "print(age_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed105327-d701-4f9b-ab87-cc375c49b0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>service_test</th>\n",
       "      <th>age_id</th>\n",
       "      <th>race_id</th>\n",
       "      <th>gender_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86739</th>\n",
       "      <td>train/86740.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86740</th>\n",
       "      <td>train/86741.jpg</td>\n",
       "      <td>10-19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86741</th>\n",
       "      <td>train/86742.jpg</td>\n",
       "      <td>more than 70</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86742</th>\n",
       "      <td>train/86743.jpg</td>\n",
       "      <td>10-19</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86743</th>\n",
       "      <td>train/86744.jpg</td>\n",
       "      <td>40-49</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86744 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file           age  gender        race  service_test  \\\n",
       "0          train/1.jpg         50-59    Male  East Asian          True   \n",
       "1          train/2.jpg         30-39  Female      Indian         False   \n",
       "2          train/3.jpg           3-9  Female       Black         False   \n",
       "3          train/4.jpg         20-29  Female      Indian          True   \n",
       "4          train/5.jpg         20-29  Female      Indian          True   \n",
       "...                ...           ...     ...         ...           ...   \n",
       "86739  train/86740.jpg         20-29    Male      Indian          True   \n",
       "86740  train/86741.jpg         10-19    Male      Indian          True   \n",
       "86741  train/86742.jpg  more than 70  Female      Indian          True   \n",
       "86742  train/86743.jpg         10-19  Female       Black          True   \n",
       "86743  train/86744.jpg         40-49    Male       White         False   \n",
       "\n",
       "       age_id  race_id  gender_id  \n",
       "0           6        1          1  \n",
       "1           4        2          0  \n",
       "2           3        0          0  \n",
       "3           2        2          0  \n",
       "4           2        2          0  \n",
       "...       ...      ...        ...  \n",
       "86739       2        2          1  \n",
       "86740       1        2          1  \n",
       "86741       8        2          0  \n",
       "86742       1        0          0  \n",
       "86743       5        6          1  \n",
       "\n",
       "[86744 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c06d9-c8e9-42a4-a2da-da50388e7da0",
   "metadata": {},
   "source": [
    "# å­¦ç¿’ã‚³ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b776ed41-e7d7-413a-9f71-dc6cca65ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.stats import multivariate_normal\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    AveragePooling2D,\n",
    "    MaxPooling2D,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    "    MaxPool2D,\n",
    "    Reshape,\n",
    "    UpSampling2D,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b069fb4-b366-4da3-bbf0-3501b42956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "NUM_CLASSES_GENDER=2\n",
    "NUM_CLASSES_RACE=7\n",
    "NUM_CLASSES_AGE=9\n",
    "EPOCHS=30\n",
    "INPUT_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "out_root = \"./results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b6e7b54-4540-466f-bc94-592b65ea8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "tz_jst = timezone(timedelta(hours=9))\n",
    "train_start_time = datetime.now(tz=tz_jst).strftime('%Y%m%d_%H%M')\n",
    "\n",
    "out_root = out_root + train_start_time +  \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa003e3c-96d4-42bf-8e7f-22fd9068585c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fbaa798-6f97-44d4-ab68-fb09bc5fbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_paths = train_df[\"file\"].values.tolist()\n",
    "y = train_df[\"race\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1b7fc1-38e6-42dd-99ec-8e5c33d6d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_paths, y, test_size=0.2, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03d591ef-1b1f-4e7f-88d3-90248083caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"is_train\"] = train_df['file'].isin(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e85f3d-f3fe-4b9e-8bcf-681b6e1b1f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4a35324-80cd-4e3e-8daa-67b7987ec3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "info[\"num_classes_gender\"] = NUM_CLASSES_GENDER\n",
    "info[\"num_classes_race\"] = NUM_CLASSES_RACE\n",
    "info[\"num_classes_age\"] = NUM_CLASSES_AGE\n",
    "info[\"input_shape\"] = INPUT_SIZE\n",
    "info[\"batch_size\"] = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6c848ce-ef03-44f1-95ae-fb1c531a17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        A.ShiftScaleRotate(p=0.5, shift_limit=0.0625, rotate_limit=(-5, 5)),\n",
    "        A.RandomResizedCrop(\n",
    "            width=info[\"input_shape\"][1],\n",
    "            height=info[\"input_shape\"][0],\n",
    "            scale=(0.9, 1.1),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8ed1ab8-d56d-43a5-bf7d-e124e45f6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self, df, subset=\"train\", shuffle=False, preprocess=None, info={}\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.paths = df[\"file\"].values\n",
    "        self.labels_age = df[\"age_id\"].values\n",
    "        self.labels_race = df[\"race_id\"].values\n",
    "        self.labels_gender = df[\"gender_id\"].values\n",
    "\n",
    "        self.subset = subset\n",
    "        self.shuffle = shuffle\n",
    "        self.preprocess = preprocess\n",
    "        self.info = info\n",
    "        self.num_class_gender = info[\"num_classes_gender\"] \n",
    "        self.num_class_race = info[\"num_classes_race\"] \n",
    "        self.num_class_age = info[\"num_classes_age\"] \n",
    "        self.input_shape = info[\"input_shape\"]\n",
    "        self.batch_size = info[\"batch_size\"]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.paths) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X = np.empty(\n",
    "            (self.batch_size, self.input_shape[0], self.input_shape[1], 3),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        y_age = np.zeros((self.batch_size, self.num_class_age), dtype=np.uint8)\n",
    "        y_race = np.zeros((self.batch_size, self.num_class_race), dtype=np.uint8)\n",
    "        y_gender = np.zeros((self.batch_size, self.num_class_gender), dtype=np.uint8)\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        \n",
    "        \n",
    "        for i, idx in enumerate(indexes):\n",
    "            # images\n",
    "            img_path = self.paths[idx]\n",
    "            image = (np.array(Image.open(img_path).convert(\"RGB\")) / 255).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.stack([image, image, image], 2)\n",
    "            \n",
    "            # labels\n",
    "            y_age[i, self.labels_age[idx]] = 1\n",
    "            y_race[i, self.labels_race[idx]] = 1\n",
    "            y_gender[i, self.labels_gender[idx]] = 1\n",
    "\n",
    "            # ================================\n",
    "            # preprocessing with albumentations\n",
    "            # ================================\n",
    "            if self.preprocess != None:\n",
    "                transformed = self.preprocess(image=image)\n",
    "                X[i,] = transformed[\n",
    "                    \"image\"\n",
    "                ].astype(np.float32)\n",
    "            else:\n",
    "                X[i,] = cv2.resize(\n",
    "                    image,\n",
    "                    dsize=(self.input_shape[1], self.input_shape[0]),\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                ).astype(np.float32)\n",
    "\n",
    "        \n",
    "        # ================================\n",
    "        # return\n",
    "        # ================================\n",
    "        return X.astype(np.float32), [y_age.astype(np.float32), y_race.astype(np.float32), y_gender.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db2aebe3-d78a-4dfe-a060-da5ccd5daa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagenerator = DataGenerator(\n",
    "    train_df[train_df[\"is_train\"]], subset=\"train\", shuffle=True, preprocess=transform, info=info\n",
    ")\n",
    "\n",
    "valid_datagenerator = DataGenerator(\n",
    "    train_df[~train_df[\"is_train\"]], subset=\"valid\", shuffle=False, preprocess=None, info=info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3f4cc3d-89ae-4d4e-b609-b37bfc8c4238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagenerator[0][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "97739988-6c5d-4077-af74-0eaa511a04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMobileNetV2(n_classes, input_size=(224, 224, 3)):\n",
    "    \"\"\"Input\"\"\"\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    # model = EfficientNetB0(weights='imagenet')\n",
    "    \n",
    "    encoder = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
    "    h = Flatten()(encoder.output)\n",
    "    model_output_age = Dense(n_classes[0], activation=\"softmax\", name='age_output')(h)\n",
    "    model_output_race = Dense(n_classes[1], activation=\"softmax\", name='race_output')(h)\n",
    "    model_output_gender = Dense(n_classes[2], activation=\"softmax\", name='gender_output')(h)\n",
    "\n",
    "    model = Model(inputs, [model_output_age, model_output_race, model_output_gender])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "27befb44-024f-4f95-aa0a-896849b619fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/30\n",
      "4337/4337 [==============================] - 329s 75ms/step - loss: 4.5043 - age_output_loss: 1.9243 - race_output_loss: 1.9254 - gender_output_loss: 0.6546 - age_output_accuracy: 0.3060 - race_output_accuracy: 0.3096 - gender_output_accuracy: 0.6833 - val_loss: 3.8839 - val_age_output_loss: 1.6778 - val_race_output_loss: 1.6647 - val_gender_output_loss: 0.5415 - val_age_output_accuracy: 0.3536 - val_race_output_accuracy: 0.3759 - val_gender_output_accuracy: 0.7422\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "4337/4337 [==============================] - 324s 75ms/step - loss: 3.6459 - age_output_loss: 1.5838 - race_output_loss: 1.5616 - gender_output_loss: 0.5005 - age_output_accuracy: 0.3695 - race_output_accuracy: 0.4037 - gender_output_accuracy: 0.7547 - val_loss: 3.4604 - val_age_output_loss: 1.5133 - val_race_output_loss: 1.4861 - val_gender_output_loss: 0.4610 - val_age_output_accuracy: 0.3948 - val_race_output_accuracy: 0.4331 - val_gender_output_accuracy: 0.7784\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 3/30\n",
      "4337/4337 [==============================] - 324s 75ms/step - loss: 3.3587 - age_output_loss: 1.4758 - race_output_loss: 1.4394 - gender_output_loss: 0.4435 - age_output_accuracy: 0.3995 - race_output_accuracy: 0.4497 - gender_output_accuracy: 0.7858 - val_loss: 3.2242 - val_age_output_loss: 1.4292 - val_race_output_loss: 1.3823 - val_gender_output_loss: 0.4127 - val_age_output_accuracy: 0.4137 - val_race_output_accuracy: 0.4727 - val_gender_output_accuracy: 0.8040\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 4/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 3.1761 - age_output_loss: 1.4092 - race_output_loss: 1.3605 - gender_output_loss: 0.4065 - age_output_accuracy: 0.4248 - race_output_accuracy: 0.4782 - gender_output_accuracy: 0.8062 - val_loss: 3.0886 - val_age_output_loss: 1.3828 - val_race_output_loss: 1.3220 - val_gender_output_loss: 0.3838 - val_age_output_accuracy: 0.4193 - val_race_output_accuracy: 0.4970 - val_gender_output_accuracy: 0.8207\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 5/30\n",
      "4337/4337 [==============================] - 324s 75ms/step - loss: 3.0507 - age_output_loss: 1.3619 - race_output_loss: 1.3040 - gender_output_loss: 0.3849 - age_output_accuracy: 0.4401 - race_output_accuracy: 0.4997 - gender_output_accuracy: 0.8178 - val_loss: 3.0730 - val_age_output_loss: 1.3744 - val_race_output_loss: 1.3209 - val_gender_output_loss: 0.3777 - val_age_output_accuracy: 0.4272 - val_race_output_accuracy: 0.5003 - val_gender_output_accuracy: 0.8237\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 6/30\n",
      "4337/4337 [==============================] - 327s 75ms/step - loss: 2.9433 - age_output_loss: 1.3227 - race_output_loss: 1.2554 - gender_output_loss: 0.3652 - age_output_accuracy: 0.4545 - race_output_accuracy: 0.5206 - gender_output_accuracy: 0.8312 - val_loss: 2.9065 - val_age_output_loss: 1.3174 - val_race_output_loss: 1.2417 - val_gender_output_loss: 0.3474 - val_age_output_accuracy: 0.4480 - val_race_output_accuracy: 0.5312 - val_gender_output_accuracy: 0.8363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 7/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.8548 - age_output_loss: 1.2887 - race_output_loss: 1.2211 - gender_output_loss: 0.3449 - age_output_accuracy: 0.4670 - race_output_accuracy: 0.5335 - gender_output_accuracy: 0.8423 - val_loss: 2.8398 - val_age_output_loss: 1.2913 - val_race_output_loss: 1.2164 - val_gender_output_loss: 0.3320 - val_age_output_accuracy: 0.4634 - val_race_output_accuracy: 0.5393 - val_gender_output_accuracy: 0.8465\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 8/30\n",
      "4337/4337 [==============================] - 327s 75ms/step - loss: 2.7823 - age_output_loss: 1.2628 - race_output_loss: 1.1863 - gender_output_loss: 0.3333 - age_output_accuracy: 0.4775 - race_output_accuracy: 0.5496 - gender_output_accuracy: 0.8486 - val_loss: 2.7738 - val_age_output_loss: 1.2719 - val_race_output_loss: 1.1802 - val_gender_output_loss: 0.3218 - val_age_output_accuracy: 0.4596 - val_race_output_accuracy: 0.5536 - val_gender_output_accuracy: 0.8536\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 9/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.7177 - age_output_loss: 1.2364 - race_output_loss: 1.1594 - gender_output_loss: 0.3219 - age_output_accuracy: 0.4869 - race_output_accuracy: 0.5592 - gender_output_accuracy: 0.8522 - val_loss: 2.7487 - val_age_output_loss: 1.2589 - val_race_output_loss: 1.1786 - val_gender_output_loss: 0.3111 - val_age_output_accuracy: 0.4713 - val_race_output_accuracy: 0.5566 - val_gender_output_accuracy: 0.8576\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 10/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.6677 - age_output_loss: 1.2207 - race_output_loss: 1.1364 - gender_output_loss: 0.3106 - age_output_accuracy: 0.4923 - race_output_accuracy: 0.5681 - gender_output_accuracy: 0.8594 - val_loss: 2.6995 - val_age_output_loss: 1.2488 - val_race_output_loss: 1.1496 - val_gender_output_loss: 0.3011 - val_age_output_accuracy: 0.4649 - val_race_output_accuracy: 0.5646 - val_gender_output_accuracy: 0.8655\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 11/30\n",
      "4337/4337 [==============================] - 324s 75ms/step - loss: 2.6134 - age_output_loss: 1.2025 - race_output_loss: 1.1105 - gender_output_loss: 0.3005 - age_output_accuracy: 0.5008 - race_output_accuracy: 0.5778 - gender_output_accuracy: 0.8636 - val_loss: 2.6604 - val_age_output_loss: 1.2318 - val_race_output_loss: 1.1327 - val_gender_output_loss: 0.2958 - val_age_output_accuracy: 0.4757 - val_race_output_accuracy: 0.5702 - val_gender_output_accuracy: 0.8690\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 12/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.5655 - age_output_loss: 1.1814 - race_output_loss: 1.0891 - gender_output_loss: 0.2950 - age_output_accuracy: 0.5080 - race_output_accuracy: 0.5861 - gender_output_accuracy: 0.8687 - val_loss: 2.6271 - val_age_output_loss: 1.2212 - val_race_output_loss: 1.1160 - val_gender_output_loss: 0.2899 - val_age_output_accuracy: 0.4782 - val_race_output_accuracy: 0.5809 - val_gender_output_accuracy: 0.8704\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 13/30\n",
      "4337/4337 [==============================] - 333s 77ms/step - loss: 2.5191 - age_output_loss: 1.1656 - race_output_loss: 1.0691 - gender_output_loss: 0.2843 - age_output_accuracy: 0.5163 - race_output_accuracy: 0.5935 - gender_output_accuracy: 0.8733 - val_loss: 2.6089 - val_age_output_loss: 1.2152 - val_race_output_loss: 1.1103 - val_gender_output_loss: 0.2835 - val_age_output_accuracy: 0.4828 - val_race_output_accuracy: 0.5807 - val_gender_output_accuracy: 0.8744\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 14/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.4837 - age_output_loss: 1.1526 - race_output_loss: 1.0510 - gender_output_loss: 0.2800 - age_output_accuracy: 0.5205 - race_output_accuracy: 0.6020 - gender_output_accuracy: 0.8750 - val_loss: 2.5883 - val_age_output_loss: 1.2153 - val_race_output_loss: 1.0924 - val_gender_output_loss: 0.2806 - val_age_output_accuracy: 0.4772 - val_race_output_accuracy: 0.5901 - val_gender_output_accuracy: 0.8756\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 15/30\n",
      "4337/4337 [==============================] - 323s 75ms/step - loss: 2.4518 - age_output_loss: 1.1404 - race_output_loss: 1.0392 - gender_output_loss: 0.2722 - age_output_accuracy: 0.5250 - race_output_accuracy: 0.6071 - gender_output_accuracy: 0.8788 - val_loss: 2.5696 - val_age_output_loss: 1.2022 - val_race_output_loss: 1.0894 - val_gender_output_loss: 0.2779 - val_age_output_accuracy: 0.4859 - val_race_output_accuracy: 0.5886 - val_gender_output_accuracy: 0.8781\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 16/30\n",
      "4337/4337 [==============================] - 327s 75ms/step - loss: 2.4115 - age_output_loss: 1.1261 - race_output_loss: 1.0175 - gender_output_loss: 0.2679 - age_output_accuracy: 0.5340 - race_output_accuracy: 0.6136 - gender_output_accuracy: 0.8812 - val_loss: 2.5404 - val_age_output_loss: 1.1958 - val_race_output_loss: 1.0735 - val_gender_output_loss: 0.2712 - val_age_output_accuracy: 0.4869 - val_race_output_accuracy: 0.5976 - val_gender_output_accuracy: 0.8802\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 17/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.3821 - age_output_loss: 1.1170 - race_output_loss: 1.0049 - gender_output_loss: 0.2602 - age_output_accuracy: 0.5343 - race_output_accuracy: 0.6186 - gender_output_accuracy: 0.8854 - val_loss: 2.5393 - val_age_output_loss: 1.1952 - val_race_output_loss: 1.0706 - val_gender_output_loss: 0.2735 - val_age_output_accuracy: 0.4875 - val_race_output_accuracy: 0.5936 - val_gender_output_accuracy: 0.8794\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 18/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.3492 - age_output_loss: 1.1013 - race_output_loss: 0.9884 - gender_output_loss: 0.2595 - age_output_accuracy: 0.5423 - race_output_accuracy: 0.6263 - gender_output_accuracy: 0.8840 - val_loss: 2.5061 - val_age_output_loss: 1.1829 - val_race_output_loss: 1.0583 - val_gender_output_loss: 0.2649 - val_age_output_accuracy: 0.4919 - val_race_output_accuracy: 0.6027 - val_gender_output_accuracy: 0.8835\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 19/30\n",
      "4337/4337 [==============================] - 324s 75ms/step - loss: 2.3190 - age_output_loss: 1.0907 - race_output_loss: 0.9765 - gender_output_loss: 0.2518 - age_output_accuracy: 0.5452 - race_output_accuracy: 0.6308 - gender_output_accuracy: 0.8894 - val_loss: 2.5034 - val_age_output_loss: 1.1785 - val_race_output_loss: 1.0610 - val_gender_output_loss: 0.2639 - val_age_output_accuracy: 0.5012 - val_race_output_accuracy: 0.6015 - val_gender_output_accuracy: 0.8849\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 20/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.2863 - age_output_loss: 1.0757 - race_output_loss: 0.9646 - gender_output_loss: 0.2460 - age_output_accuracy: 0.5526 - race_output_accuracy: 0.6346 - gender_output_accuracy: 0.8921 - val_loss: 2.4957 - val_age_output_loss: 1.1783 - val_race_output_loss: 1.0561 - val_gender_output_loss: 0.2612 - val_age_output_accuracy: 0.5001 - val_race_output_accuracy: 0.6054 - val_gender_output_accuracy: 0.8893\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 21/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.2644 - age_output_loss: 1.0699 - race_output_loss: 0.9504 - gender_output_loss: 0.2441 - age_output_accuracy: 0.5550 - race_output_accuracy: 0.6408 - gender_output_accuracy: 0.8923 - val_loss: 2.4979 - val_age_output_loss: 1.1802 - val_race_output_loss: 1.0580 - val_gender_output_loss: 0.2597 - val_age_output_accuracy: 0.4955 - val_race_output_accuracy: 0.6027 - val_gender_output_accuracy: 0.8858\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 22/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.2327 - age_output_loss: 1.0548 - race_output_loss: 0.9396 - gender_output_loss: 0.2384 - age_output_accuracy: 0.5623 - race_output_accuracy: 0.6437 - gender_output_accuracy: 0.8955 - val_loss: 2.4859 - val_age_output_loss: 1.1809 - val_race_output_loss: 1.0446 - val_gender_output_loss: 0.2605 - val_age_output_accuracy: 0.4957 - val_race_output_accuracy: 0.6094 - val_gender_output_accuracy: 0.8875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 23/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.2108 - age_output_loss: 1.0471 - race_output_loss: 0.9305 - gender_output_loss: 0.2331 - age_output_accuracy: 0.5674 - race_output_accuracy: 0.6483 - gender_output_accuracy: 0.8993 - val_loss: 2.4737 - val_age_output_loss: 1.1734 - val_race_output_loss: 1.0380 - val_gender_output_loss: 0.2624 - val_age_output_accuracy: 0.5003 - val_race_output_accuracy: 0.6124 - val_gender_output_accuracy: 0.8851\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 24/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.1868 - age_output_loss: 1.0394 - race_output_loss: 0.9168 - gender_output_loss: 0.2305 - age_output_accuracy: 0.5672 - race_output_accuracy: 0.6539 - gender_output_accuracy: 0.8999 - val_loss: 2.4517 - val_age_output_loss: 1.1696 - val_race_output_loss: 1.0277 - val_gender_output_loss: 0.2544 - val_age_output_accuracy: 0.5051 - val_race_output_accuracy: 0.6149 - val_gender_output_accuracy: 0.8914\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 25/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.1644 - age_output_loss: 1.0266 - race_output_loss: 0.9089 - gender_output_loss: 0.2290 - age_output_accuracy: 0.5752 - race_output_accuracy: 0.6578 - gender_output_accuracy: 0.8993 - val_loss: 2.4473 - val_age_output_loss: 1.1716 - val_race_output_loss: 1.0233 - val_gender_output_loss: 0.2524 - val_age_output_accuracy: 0.4972 - val_race_output_accuracy: 0.6197 - val_gender_output_accuracy: 0.8912\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 26/30\n",
      "4337/4337 [==============================] - 325s 75ms/step - loss: 2.1424 - age_output_loss: 1.0215 - race_output_loss: 0.8988 - gender_output_loss: 0.2221 - age_output_accuracy: 0.5780 - race_output_accuracy: 0.6615 - gender_output_accuracy: 0.9033 - val_loss: 2.4609 - val_age_output_loss: 1.1880 - val_race_output_loss: 1.0229 - val_gender_output_loss: 0.2500 - val_age_output_accuracy: 0.5021 - val_race_output_accuracy: 0.6181 - val_gender_output_accuracy: 0.8929\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 27/30\n",
      "4337/4337 [==============================] - 326s 75ms/step - loss: 2.1174 - age_output_loss: 1.0117 - race_output_loss: 0.8869 - gender_output_loss: 0.2187 - age_output_accuracy: 0.5831 - race_output_accuracy: 0.6652 - gender_output_accuracy: 0.9043 - val_loss: 2.4254 - val_age_output_loss: 1.1626 - val_race_output_loss: 1.0149 - val_gender_output_loss: 0.2479 - val_age_output_accuracy: 0.5116 - val_race_output_accuracy: 0.6235 - val_gender_output_accuracy: 0.8923\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,age_output_loss,race_output_loss,gender_output_loss,age_output_accuracy,race_output_accuracy,gender_output_accuracy,val_loss,val_age_output_loss,val_race_output_loss,val_gender_output_loss,val_age_output_accuracy,val_race_output_accuracy,val_gender_output_accuracy\n",
      "Epoch 28/30\n",
      "1932/4337 [============>.................] - ETA: 2:44 - loss: 2.0985 - age_output_loss: 0.9960 - race_output_loss: 0.8837 - gender_output_loss: 0.2188 - age_output_accuracy: 0.5871 - race_output_accuracy: 0.6664 - gender_output_accuracy: 0.9031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = myMobileNetV2(n_classes=[ NUM_CLASSES_AGE, NUM_CLASSES_RACE, NUM_CLASSES_GENDER])\n",
    "steps_per_epoch = np.ceil((len(X_train)) / BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil((len(X_val)) / BATCH_SIZE)\n",
    "\n",
    "if True:\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", mode=\"max\", verbose=1, patience=10\n",
    "    )  # Early stopping (stops training when validation doesn't improve for {patience} epochs)\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f\"{out_root}myMobileNetV2.h5\", monitor=\"val_race_output_accuracy\", save_best_only=True, mode=\"max\"\n",
    "    )  # Saves the best version of the model to disk (as measured on the validation data set)\n",
    "    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_race_output_accuracy\", patience=2, verbose=1, factor=0.1\n",
    "    )\n",
    "\n",
    "    # Warming up\n",
    "    model.compile(optimizer=SGD(learning_rate=1e-4), \n",
    "              loss={'age_output': 'categorical_crossentropy', 'race_output': 'categorical_crossentropy', 'gender_output': 'categorical_crossentropy'},\n",
    "              metrics={'age_output': 'accuracy', 'race_output': 'accuracy', 'gender_output': 'accuracy'})\n",
    "\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_datagenerator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_datagenerator,\n",
    "        callbacks=[es, save_best, learning_rate_reduction],\n",
    "    )\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.savefig(f\"{out_root}loss.jpg\")\n",
    "    plt.show()\n",
    "    model.load_weights(f\"{out_root}myMobileNetV2.h5\")\n",
    "else:\n",
    "    model.load_weights(f\"{out_root}myMobileNetV2.h5\")\n",
    "    # model.evaluate(val_datagen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa941d-bfaa-4132-9252-cb37a930310b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
